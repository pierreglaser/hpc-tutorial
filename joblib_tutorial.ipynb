{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction: `joblib` tutorial\n",
    "\n",
    "In this tutorial, I will walk you through `joblib`, which is a package that provides\n",
    "- an fast/easy/robust way to parallelize computations using `joblib.Parallel`\n",
    "- computation caching using `joblib.Memory`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributing code on a single machine using `joblib.Parallel`\n",
    "\n",
    "`joblib` is a pure-python package that allows you to run `Python` code in parallel.\n",
    "\n",
    "Scientific numerical experiments often require running the **same function**, **multiple times** using a different set of parameters each time (well known examples include cross-validation, or hyper-parameter selection)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A simple example: executing multiple function calls in serial mode\n",
    "\n",
    "Let's start with a very simple example: I want to run `my_fuction` multiple times for different sets of arguments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "# Here is the function that I want to parallelize -- a more realistic\n",
    "# example would be named ``fit_model`` for instance.\n",
    "def my_function(i):\n",
    "    \n",
    "    # Simulate a long-running computation. A real-life code could be np.linalg.svd(...).\n",
    "    time.sleep(0.5)\n",
    "    \n",
    "    return i\n",
    "\n",
    "\n",
    "# The set of arguments I want to run my function on. Each item in this list could\n",
    "# be the value of a regularization parameter\n",
    "my_args = list(range(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "results = [my_function(arg) for arg in my_args]\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell above took ~ 5 seconds to run -- it ran all of theese function calls one after the other,\n",
    "which can be suboptimal when a computer has multiple processing units (CPUs), which is pretty much the case for all modern computers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Executing multiple function calls in parallel using `joblib`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "results = Parallel(n_jobs=2)(delayed(my_function)(arg) for arg in my_args)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **exact** same code runs now on ~ 2.5 seconds, which is ~ half the time it took in the serial situation!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Advanced note: in standard CPU architectures, code in parallel is done using either **threads** or **processes**\n",
    "By default, `joblib` relies on **processes**-based parallelism, because **thread**-based parallelism has limitations in `Python`. You can specify which kind of parallelism you want `joblib` to use using the `backend` option of the `Parallel` constructor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "results = Parallel(n_jobs=2, backend='threading')(delayed(my_function)(arg) for arg in my_args)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Real life computations - Discussion, Pitfalls, Limits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The benchmark above is helpful to quickly demonstrate what `joblib` can do, but is not representative of a realistic situation because `my_function` does not perform any actual computation. For real-life use cases, using simple personal computers, the speed up that `joblib` will provide is dependent on your code's computational specificities. Here are a few remarks:\n",
    "\n",
    "#### Competing with lower-level parallelization\n",
    "\n",
    "Python is a language that can execute arbitrary C code, which is generally much faster to run than Python code. For instance, most of `numpy` is written in C. When possible, `numpy` parallelizes matrix operations at a low level using linear algebra libraries such as `OpenBLAS` or `MKL`. This **low-level** parallelization competes with the **higher-level** parallelization that what joblib relies on. Using both parallelization level at the same time can lead to **over-subscription** (too many instructions threads executed concurrenltly), which can **decrease** your code performance, especially on machines with manu `CPU`s.\n",
    "\n",
    "\n",
    "Luckly, both parallelism can be enabled, disabled using methods that I will discuss below. Here is how switchin on/off each parallelism performs on a standard numpy workload:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](benchmarks/results.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Note: hese benchmarks are run on a 24 physical/48 virtual CPU machine. In the `inner` case, the BLAS library is free to use any number of threads. In the `outer` case, `joblib` spawns 48 workers (which is un-necessary when the number of sdv run is below 48, but I decided to keep the resource logic simple), and I specifically ask the BLAS library to use only 1 thread.\n",
    "\n",
    "As you can see, when a few outer tasks (svd) are computed, inner-parallelism takes advantage of the 48 CPUs while `joblib` cannot. But as the number of tasks increases, the inner method scales less well than the joblib one.\n",
    "\n",
    "Keep in mind that this scenario in a close to worst-case scenario in `joblib` as there is a lot of complexity happening outdside of the `Python` interpreter (most of the work is done by low-level `C` routines \"outside\" of Python, ask me later if you want to know what that means)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the function I ran, and a small benchmark on my machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def run_svd(random_seed):\n",
    "    rs = np.random.RandomState(random_seed)\n",
    "    array = rs.randn(1500, 1500)\n",
    "    U, S, V = np.linalg.svd(array)\n",
    "    return S[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "results = [run_svd(seed) for seed in range(4)]\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "results = Parallel(n_jobs=2)(delayed(run_svd)(seed) for seed in range(4))\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to disable inner parallelism:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using `joblib`, it is recommended to disable lower-level serialization, by defining/exporting the following environment variables:\n",
    "```bash\n",
    "OPENBLAS_NUM_THREADS=1  # if numpy uses openblas\n",
    "MKL_NUM_THREAD=1        # if numpy uses MKL. Will be the case for a numpy coming from `conda`\n",
    "```\n",
    "\n",
    "You can either put these lines in your favorite shell configuration file (`~/.bashrc`/`~/.bash_profile`/`~/.profile`...)\n",
    "```bash\n",
    "export OPENBLAS_NUM_THREADS=1  # if numpy uses openblas\n",
    "export MKL_NUM_THREAD=1        # if numpy uses MKL. Will be the case for a numpy coming from `conda`\n",
    "```\n",
    "\n",
    "Or use them in a case by case basis:\n",
    "```\n",
    "OPENBLAS_NUM_THREADS=1  MKL_NUM_THREAD=1 jupyter-notebook  # start a jupyter-notebook in which blas parallelization is disabled\n",
    "```\n",
    "\n",
    "\n",
    "**Advanced note**\n",
    "Enabling/disabling blas can now be done programatically for each chunk of code using our new `threadpoolctl` package. This is in early development, and only recommended for \"experts\".\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Advanced note: `numpy` functions generally release the Python Global Interpreter Lock (GIL), which means that\n",
    "thread-based parallelization is likely to be as efficient as process-based parallelization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "results = Parallel(backend='threading', n_jobs=2)(delayed(run_svd)(seed) for seed in range(4))\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A better use-case for `joblib`: Python-level iterative algorithms\n",
    "Not all Python code is efficient numpy code - iterative algorithms such as gradient descent or MCMC methods generally use a Python `for`-loop. For such setups, `joblib` will be more likely to yield higher speedups:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_mcmc(seed):\n",
    "    rs = np.random.RandomState(seed)\n",
    "    n_iter = 1000000\n",
    "    theta = 0\n",
    "    for i in range(n_iter):\n",
    "        theta += rs.randn(10)\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "results = [run_mcmc(i) for i in range(4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "results = Parallel(n_jobs=2)(delayed(run_mcmc)(i) for i in range(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Advanced note: in situation involving iterative logic, another way to improve the execution performance is to use rely just-in-time compilation. A few pointers: `jax`, `numba` (and even `pypy!`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Issues preventing ideal speedups\n",
    "\n",
    "In an ideal setting, One would typically expect that `joblib` yields a linear speedup (in log-log scale) w.r.t the number of available CPUs. That is:\n",
    "\n",
    "$$\\text{running_time(n_cpu)} = \\frac{\\text{serial_running_time}}{\\text{n_cpu}}$$\n",
    "\n",
    "In practice, this is not the case, because of various low-level phenomenas:\n",
    "\n",
    "#### Resources sharing\n",
    "\n",
    "\n",
    "Modern CPUs speed-up data access using caches. Some of these caches are shared between CPUs. When running code in parallel, multiple CPUs will compete to use the same shared cache, which tends to slow things down.\n",
    "\n",
    "#### Interprocess communication\n",
    "\n",
    "process-based parallelism is usually the recommended parallelism form in Python as thread-based parallelism can be compromised by the GIL. However, processes do not share memory, so child processes must be transmitted data (the function, the inputs...) from the original process. Serializing data is done in serial. It is a fixed time addition whose importance w.r.t the total running time increases as the number of CPUs increases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "![title](benchmarks/results_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatic caching using `joblib.Memory`\n",
    "\n",
    "\n",
    "Among the main pain points of numerical experiments, there is one that stands out: having a long-running experiment erroring out during its execution. Usually, intermediate computations will be lost, which can mean days of computing and human work wasted.\n",
    "\n",
    "`joblib` provides a way to cache a function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Memory\n",
    "memory = Memory('joblib-cache-directory')\n",
    "\n",
    "# caching functions defined inside notebooks is tricky. For maximum\n",
    "# robustness, my_function should be moved to a python file when using\n",
    "# joblib.Memory\n",
    "from my_module import my_function\n",
    "my_function_cached = memory.cache(my_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "results = Parallel(n_jobs=2)(delayed(my_function_cached)(arg) for arg in my_args)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "results = Parallel(n_jobs=2)(delayed(my_function_cached)(arg) for arg in my_args)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the drop in execution time between the two cells above? In the first cell, my_function did not have\n",
    "any cached results yet; executing those function calls took the same time as in the not-cached case.\n",
    "\n",
    "But in the next cells, the exact same `Parallel` call took only 10ms: `joblib` did not re-execute all of these function calls, but only loaded the already-computed results!\n",
    "\n",
    "**Important remarks**: each time the source codes of a `joblib.Memory`-cached function changes, the cached associated to this function is cleared."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From single-machine parallelism to multi-machine parallelism\n",
    "\n",
    "- Usually, `joblib` can yield improvements proportional to the number of CPUs of the machine you are using. If your machine has 4 CPUs, you can expect the total running time of your function calls to be divided by up to 4 as compared to the serial case. Usually, laptops have from 2 up to 12 CPUs.\n",
    "\n",
    "- `joblib` provides a way to quickly parallelize code on a single machine. But scientific institutions such as our often have access to a larger set of computational resources, such as a slurm cluster. Running code in various nodes of a slurm cluster in parallel can be done in various ways (submitting `batch` scripts for instance). But doing this is error prone, and not the areas of expertise of researchers. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
