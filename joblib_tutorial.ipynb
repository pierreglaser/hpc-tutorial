{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributing code on a single machine using `joblib`\n",
    "\n",
    "`joblib` is a pure-python package that allows you to run `Python` code in parallel.\n",
    "\n",
    "Scientific numerical experiments often require running the **same function**, **multiple times** using a different set of parameters each time (well known examples include cross-validation, or hyper-parameter selection)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A simple example: executing multiple function calls in serial mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "# Here is the function that I want to parallelize -- a more realistic\n",
    "# example would be named ``fit_model`` for instance.\n",
    "def my_function(i):\n",
    "    \n",
    "    # Simulate a long-running computation. A real-life code could be np.linalg.svd(...).\n",
    "    time.sleep(i)\n",
    "    \n",
    "    return i\n",
    "\n",
    "\n",
    "# The set of arguments I want to run my function on. Each item in this list could\n",
    "# be the value of a regularization parameter\n",
    "my_args = [0.5, 1, 1.5, 2, 2.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "results = []\n",
    "for arg in my_args:\n",
    "    results.append(my_function(arg))\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell above took ~ 7.5 seconds to run -- it ran all of theese function calls one after the other,\n",
    "which is suboptimal when a computer has multiple processing units (CPUs), which is pretty much the case for all modern computers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Executing multiple function calls in parallel using `joblib`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "results = Parallel(n_jobs=2)(delayed(my_function)(arg) for arg in my_args)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **exact** same code runs now on ~ 4.5 seconds, which is ~ half the time it took in the serial situation!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Advanced note: in standard CPU architectures, code in parallel is done using either **threads** or **processes**\n",
    "By default, `joblib` relies on **processes**-based parallelism, because **thread**-based parallelism has limitations in `Python`. You can specify which kind of parallelism you want `joblib` to use using the `backend` option of the `Parallel` constructor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "results = Parallel(n_jobs=2, backend='threading')(delayed(my_function)(arg) for arg in my_args)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automatic caching using `joblib.Memory`\n",
    "\n",
    "Among the main pain points of numerical experiments, there is one that stands out: having a long-running experiment erroring out during its execution. Usually, intermediate computations will be lost, which can mean days of computing and human work wasted.\n",
    "\n",
    "`joblib` provides a way to cache a function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Memory\n",
    "memory = Memory('joblib-cache-directory')\n",
    "\n",
    "# caching functions defined inside notebooks is tricky. For maximum\n",
    "# robustness, my_function should be moved to a python file when using\n",
    "# joblib.Memory\n",
    "from my_module import my_function\n",
    "my_function_cached = memory.cache(my_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "results = Parallel(n_jobs=2)(delayed(my_function_cached)(arg) for arg in my_args)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "results = Parallel(n_jobs=2)(delayed(my_function_cached)(arg) for arg in my_args)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the drop in execution time between the two cells above? In the first cell, my_function did not have\n",
    "any cached results yet; executing those function calls took the same time as in the not-cached case.\n",
    "\n",
    "But in the next cells, the exact same `Parallel` call took only 10ms: `joblib` did not re-execute all of these function calls, but only loaded the already-computed results!\n",
    "\n",
    "**Important remarks**: each time the source codes of a `joblib.Memory`-cached function changes, the cached associated to this function is cleared."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From single-machine parallelism to multi-machine parallelism\n",
    "\n",
    "- Usually, `joblib` can yield improvements proportional to the number of CPUs of the machine you are using. If your machine has 4 CPUs, you can expect the total running time of your function calls to be divided by up to 4 as compared to the serial case. Usually, laptops have from 2 up to 12 CPUs.\n",
    "\n",
    "- `joblib` provides a way to quickly parallelize code on a single machine. But scientific institutions such as our often have access to a larger set of computational resources, such as a slurm cluster. Running code in various nodes of a slurm cluster in parallel can be done in various ways (submitting `batch` scripts for instance). But doing this is error prone, and not the areas of expertise of researchers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Natively scaling python code using `dask`\n",
    "\n",
    "- The canonical package in the `Python` data science ecosysteAm to run `Python` code on a cluster of machine is `dask`. As opposed to `slurm` commmand lines utility, dask scales your `Python` code **natively**: no need to get out of your `jupyter` notebook!\n",
    "- `joblib` integrates with `dask`, making scaling from a single machine to a HPC cluster as seamless as possible: the only additional code you need to add is the specifications of the slurm nodes you want to use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
