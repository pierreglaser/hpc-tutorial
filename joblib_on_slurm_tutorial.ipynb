{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natively scaling python code using `dask`\n",
    "\n",
    "- The canonical package in the `Python` data science ecosystem to run `Python` code on a cluster of machine is `dask`. As opposed to `slurm` commmand lines utility, dask scales your `Python` code **natively**: no need to get out of your `jupyter` notebook!\n",
    "- `joblib` integrates with `dask`, making scaling from a single machine to a HPC cluster as seamless as possible: the only additional code you need to add is the specifications of the slurm nodes you want to use:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Allocate and launch compute resources using dask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# allocate the computing resources outside of joblib\n",
    "\n",
    "# dask-jobqueue provide some utilies to create and launch compute resources for\n",
    "# a variety of HPC clusters, including slurm\n",
    "from dask_jobqueue import SLURMCluster\n",
    "\n",
    "cluster = SLURMCluster(\n",
    "    workers=0,      # create the workers \"lazily\" (upon cluster.scal)\n",
    "    memory='16GB',  # amount of RAM per worker\n",
    "    processes=1,    # number of execution units per worker (threads and processes)\n",
    "    cores=1         # among those execution units, number of processes\n",
    ")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# launch 2 dask workers with one CPU each (think sbatch --cpus=1)\n",
    "cluster.scale(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from distributed import Client\n",
    "\n",
    "# connect to the workers\n",
    "client = Client(cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Small introduction to dask's API\n",
    "\n",
    "`dask`'s `Client` exposes an API to `submit`/`map` functions calls using `futures`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from my_module import my_function\n",
    "future = client.submit(my_function, 1)\n",
    "print(future)\n",
    "print(future.result())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The joblib-dask integration: no need to use dask's API!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actually, do not need to use `dask` API to run code on a `SLURM` cluster. By asking `joblib` to use the `dask` backend, joblib will internally use `dask` to distribute computation on the\n",
    "previously created `SLURMCluster`! \n",
    "\n",
    "Having access to a large `SLURMCluster`, we can launch many workers, and have impressive speed improvements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_args = list(range(1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "results = []\n",
    "for arg in my_args[:10]:\n",
    "    results.append(my_function(arg))\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale the cluster: create 50 workers\n",
    "cluster.scale(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed, parallel_backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "with parallel_backend('dask'):\n",
    "    results = Parallel()(delayed(my_function)(arg) for arg in my_args)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Caching large distributed computations using `joblib.Memory` \n",
    "\n",
    "Of course, you can use `joblib.Memory` (`joblib`'s caching feature) to cache computations on the `cluster`\n",
    "\n",
    "\n",
    "Make sure you properly specify the cache location of your `Memory` object using \n",
    "```python\n",
    "Memory('/nfs/gatsbystor/your-user-name/joblib-cache')\n",
    "```\n",
    "for instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Memory\n",
    "\n",
    "memory = Memory('/nfs/gatsbystor/pierreg/joblib-cache')\n",
    "\n",
    "my_function_cached = memory.cache(my_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "with parallel_backend('dask'):\n",
    "    results = Parallel()(delayed(my_function_cached)(arg) for arg in my_args)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(my_args + list(range(1000, 1200)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from my_module import my_function_cached\n",
    "\n",
    "with parallel_backend('dask'):\n",
    "    results = Parallel()(delayed(my_function_cached)(arg) for arg in my_args)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is very useful if you want to restart your computation with additional parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from my_module import my_function_cached\n",
    "\n",
    "with parallel_backend('dask'):\n",
    "    results = Parallel()(delayed(my_function_cached)(arg) for arg in my_args + list(range(1000, 1200)))\n",
    "print(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
